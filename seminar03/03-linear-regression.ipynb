{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 Linear Models: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regressive models...\n",
    "\n",
    "![Linear regression on practice][1]\n",
    "\n",
    "[1]: https://imgs.xkcd.com/comics/curve_fitting.png\n",
    "[2]: https://xkcd.com/2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(a, b, prec=2):\n",
    "    return np.round(mean_squared_error(a, b), prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating 1d data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 5\n",
    "b = 10\n",
    "n_points = 300\n",
    "x_min = 0.5\n",
    "x_max = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = np.random.RandomState(33)\n",
    "noise = rs.normal(0, 5, (n_points, 1))\n",
    "\n",
    "X = np.linspace(x_min, x_max, n_points)[:, None]\n",
    "y = a + b * X + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.33, random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 7))\n",
    "plt.scatter(X_test, y_test, c='b', alpha=0.3, label='test points')\n",
    "plt.scatter(X_train, y_train, c='r', alpha=0.3, label='train points')\n",
    "plt.plot([0.5, 4], [10, 45], c='black', lw=1.5, label=f'$y = {a}+{b}x$');\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.title(f'MSE of an original model on test data = {mse(a+b*X_test, y_test)}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Model\n",
    "\n",
    "$$\n",
    "y = w_0 + w_1X_1 + w_2 X_2 + \\ldots + w_k X_k\n",
    "$$\n",
    "\n",
    "In case of $k= 1$, linear model becomes trivial\n",
    "\n",
    "$$\n",
    "    y = w_0 + w_1 x, \\quad x = X_1.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = reg.predict(X_train)\n",
    "y_pred_test = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train MSE:', mse(y_pred_train, y_train),\n",
    "      'Test MSE:', mse(y_pred_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot predicted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_linspace = np.linspace(0.5, 4, 200)[:, None]\n",
    "y_pred_linspace = reg.predict(X_linspace)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(X_linspace, y_pred_linspace)\n",
    "plt.scatter(X_test, y_test, alpha=.5, c='r', label='Test points');\n",
    "plt.scatter(X_train, y_train, alpha=.3, c='b', label='Train points');\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.coef_, reg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we add more (redundant) polynomial features, could we do better on train?\n",
    "\n",
    "$$\n",
    "y = w_0 + w_1X_1 + w_2 X_2 + \\ldots + w_k X_k\n",
    "$$\n",
    "\n",
    "Let $k$ to be equal 15, then\n",
    "$$\n",
    "    X_1 = x, X_2 = x^2, \\ldots, X_{15}=x^{15}, \\quad k = 15.\n",
    "$$\n",
    "\n",
    "$$\n",
    "    y = w_0 + w_1x + w_2 x^2 + \\ldots + w_{15} x^{15}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=15, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression(fit_intercept=True)\n",
    "reg.fit(X_train_poly, y_train)\n",
    "y_pred_train = reg.predict(X_train_poly)\n",
    "y_pred_test = reg.predict(X_test_poly)\n",
    "\n",
    "print('Train MSE:', mse(y_pred_train, y_train),\n",
    "      'Test MSE:', mse(y_pred_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Prediction curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_linspace_poly = poly.fit_transform(np.linspace(0.5,4,200).reshape(-1,1))\n",
    "y_pred_linspace = reg.predict(X_linspace_poly)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(np.linspace(0.5,4,200), y_pred_linspace, label='Linear regression prediction line')\n",
    "plt.scatter(X_test, y_test, alpha=.3,c='b', label = 'Test points');\n",
    "\n",
    "plt.scatter(X_train, y_train, alpha=.3, c='r', label='Train points');\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Error curve\n",
    "\n",
    "We know that originally data was produce from a linear law, but we are building a model as a polynom of 15th degree:\n",
    "\n",
    "$$y = w_0 + w_1x + w_2 x^2 + \\ldots + w_{15} x^{15} $$\n",
    "\n",
    "What if we make some of the coefficients very small (ideally coefficients corresponded to all degrees higher than 1):\n",
    "\n",
    "- Lasso\n",
    "- Ridge\n",
    "- Elastic Net (combination of Lasso and Ridge)\n",
    "\n",
    "\n",
    "These models share the same idea: if we want to make some coefficient small, let's add their norm to the optimized function, so instead of regular MSE:\n",
    "\n",
    "$$\\sum(y - (w_0 + w_1 x))^2 \\rightarrow \\min_{\\text{w.r. }w_i} $$\n",
    "\n",
    "let's optimize the following:\n",
    "\n",
    "$$\\left[\\sum_{i=1}^{n}(y - (w_0 + w_1 x))^2 + \\frac{1}{\\alpha}\\sum_{j=1}^{k} ||w_i|| \\right]\\rightarrow \\min_{\\text{w.r. }w_i} $$\n",
    "\n",
    "in case of Ridge regression the norm is l2:\n",
    "$$||w_i|| = ||w_i||_{l_2}  = w_i^2,$$\n",
    "and in case of Lasso regression the norm is l1:\n",
    "$$||w_i|| = ||w_i||_{l_1} = |w_i|$$\n",
    "\n",
    "and $\\alpha$ is a regularization coefficient. Smaller values of $\\alpha$ correspond to harder regularization.\n",
    "\n",
    "\n",
    "> Typically, you want to change $\\alpha$ on the log-scale, e.g. 0.003, 0.001, 0.03..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_error = []\n",
    "train_error = []\n",
    "weights = []\n",
    "logspace = np.logspace(-6,1,20)\n",
    "\n",
    "for alpha in logspace:\n",
    "    reg = Ridge(fit_intercept=True, alpha=alpha)\n",
    "    reg.fit(X_train_poly, y_train)\n",
    "    y_pred_train = reg.predict(X_train_poly)\n",
    "    y_pred_test = reg.predict(X_test_poly)\n",
    "    \n",
    "    train_error.append(mse(y_pred_train, y_train))\n",
    "    test_error.append(mse(y_pred_test, y_test))\n",
    "    weights.append(np.sum(reg.coef_**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y = w_0 + w_1x + w_2 x^2 + \\ldots + w_{15} x^{15} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "ax[0].plot(logspace, train_error, label='Train')\n",
    "ax[0].plot(logspace, test_error, label='Test')\n",
    "ax[0].set_xscale('log')\n",
    "ax[0].legend();\n",
    "ax[0].arrow(1e-1, 28, -(1e-1-1e-5), 0, head_width=0.5, head_length=.000004, alpha=.5);\n",
    "ax[0].annotate('Model \"complexity\" increases', [2e-5, 27], size=12, alpha=0.5);\n",
    "ax[0].set_xlabel('Regularization coefficient')\n",
    "ax[0].set_ylabel('MSE')\n",
    "\n",
    "ax[1].plot(logspace, weights, color='r', label='$l_2$ norm of the weights: $\\sum_{j=1}^{k} w_i^2$')\n",
    "ax[1].set_xscale('log')\n",
    "ax[1].legend();\n",
    "ax[1].set_ylabel('Weights norm')\n",
    "ax[1].set_xlabel('Regularization coefficient');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmin(test_error), logspace[np.argmin(test_error)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=15, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_linspace_poly = poly.fit_transform(np.linspace(0.5,4,200).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = Ridge(alpha = logspace[15], fit_intercept=True)\n",
    "reg.fit(X_train_poly, y_train)\n",
    "y_pred_linspace = reg.predict(X_linspace_poly)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(np.linspace(0.5,4,200), y_pred_linspace, label='Ridge prediction line')\n",
    "plt.scatter(X_test, y_test, alpha=.3, c='b', label='Test points');\n",
    "plt.scatter(X_train, y_train, alpha=.3, c='r', label='Train points');\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No a Silver Bullet\n",
    "\n",
    "We do not get a perfect line, but sometimes it helps, compare to Linear Regression without regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "reg = Ridge(alpha = logspace[15], fit_intercept=True)\n",
    "reg.fit(X_train_poly, y_train)\n",
    "y_pred_linspace = reg.predict(X_linspace_poly)\n",
    "w_norm_ridge = np.round(np.sum(reg.coef_**2), 2)\n",
    "\n",
    "ax[0].plot(np.linspace(0.5,4,200), y_pred_linspace,c='r',label='Ridge regression line')\n",
    "ax[0].scatter(X_test, y_test, alpha=.3, c='b', label='Test points');\n",
    "ax[0].set_title(f'Weights $l_2$ norm is {w_norm_ridge}')\n",
    "ax[0].scatter(X_train, y_train, alpha=.3, c='r', label='Train points');\n",
    "ax[0].legend()\n",
    "\n",
    "reg = LinearRegression(fit_intercept=False)\n",
    "reg.fit(X_train_poly, y_train)\n",
    "y_pred_linspace = reg.predict(X_linspace_poly)\n",
    "w_norm_lr = np.round(np.sum(reg.coef_**2), 2)\n",
    "\n",
    "ax[1].set_title(f'Weights $l_2$ norm is {w_norm_lr}')\n",
    "ax[1].plot(np.linspace(0.5,4,200), y_pred_linspace,c='r', label='Linear regression line')\n",
    "ax[1].scatter(X_test, y_test, alpha=.3,c='b', label='Test points');\n",
    "ax[1].scatter(X_train, y_train, alpha=.3, c='r', label='Train points');\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
