{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4038abe7-66ce-4eba-83d6-240df270b9f0",
   "metadata": {},
   "source": [
    "# 04 Gradient Boosting over Decision Trees (GBDT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3a4c6c-1b7d-4742-bc8b-605b326f5d91",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "#### pros\n",
    "\n",
    "- easy to explain (interpretable and simple)\n",
    "- categorical variables\n",
    "- fast\n",
    "\n",
    "### cons\n",
    "\n",
    "- high variance\n",
    "- not additive\n",
    "- in accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac5a83b-e690-41a5-b4df-a1190522bfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe7e893-a284-45d7-b21f-39a9de333030",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0774bb1a-7be3-4dc4-9470-395463edd60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly(xs: np.ndarray):\n",
    "    return xs - 2.0 * xs ** 2 + 1.2 * xs ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d369c58-4782-450c-98d7-ce6d5f45a2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c1df76-1d62-4d8b-8f42-76172f5199d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = np.random.RandomState(42)\n",
    "noise = rs.normal(scale=0.01, size=n_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2d5a71-02b4-42d0-9dc3-ea7d5c0e6982",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(0, 1, n_points)\n",
    "ys = poly(xs) + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b8f01d-963f-4d10-917d-f15e05929d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(xs, ys, label=r'$x - 2 x^2 + 1.2 x^3$')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7c4adf-db3c-4753-8e65-22da6a7f8801",
   "metadata": {},
   "source": [
    "## Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec6deb2-b39c-44d1-952c-d62c811b50ec",
   "metadata": {},
   "source": [
    "The idea is that a combination of weak estimaters performs better than a single one. The resulting estimator is called ensemble (or meta estimator). In other words, we are strive to build a linear combination \n",
    "\n",
    "$$\n",
    "    a_{meta}(x) = \\alpha_0 + \\sum_{k=1} \\alpha_k a^{(k)}_{weak}(x)\n",
    "$$\n",
    "\n",
    "with some weights $\\alpha_k$.\n",
    "\n",
    "In our example we assume $\\alpha_0 = 0$ and all $\\alpha_k$ equal to 1. In general there are multiple ways to find $\\alpha$ (e.g Nesterov momentum or Langevine diffusion for boosting). Also, we assume weak algorthms $a^{(k)}_{weak}$ to be a decision trees.\n",
    "\n",
    "We have already seen one of the approaches to build a combination of weak estimators (e.g. `RandomForestClassifier`). That time we iteratively built a new decision tree over subset of original dataset with bootstrap (bootstrap aggregation, bagging) or a subset of features. Then we make predictions with each tree out of collection and take an average prediction.\n",
    "\n",
    "There is other approach to build an ensemble: instead of fitting to original data we could fit a model to the prediction error (residual) of an estimator built on previous step. Let's try to build an ensemble for regression problem with boosting approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae3c1ed-fc30-45ef-afde-1f8c51b6a0ca",
   "metadata": {},
   "source": [
    "## Boosting Step by Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f219e1bc-e64f-484c-a94b-e1835119d551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921506b7-dce8-4ec9-8f03-426865647783",
   "metadata": {},
   "source": [
    "#### Single Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e488d3-19d9-4cfd-8caa-ba5d54384da3",
   "metadata": {},
   "source": [
    "We use here extremely sily model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9621824b-dc9e-4366-9aa8-e952b2ec43b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg1 = DecisionTreeRegressor(max_depth=1, random_state=42)\n",
    "reg1.fit(xs[:, None], ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c001078-58dc-4001-9198-6319c91b688c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_pred1 = reg.predict(xs[:, None])\n",
    "ys_err1 = ys - ys_pred1\n",
    "score = mean_squared_error(ys, ys_pred1)\n",
    "print(f'mse is {score:e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707540cf-620c-45fd-8902-1af30959ee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tree_perf(axs, xs, ys, ys_pred, ys_err):\n",
    "    ax = axs[0]\n",
    "    ax.scatter(xs, ys, c=COLORS[0], s=5, label='y')\n",
    "    ax.plot(xs, ys_pred, c=COLORS[1], label=r'$y_{pred}$')\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    \n",
    "    ax = axs[1]\n",
    "    ax.scatter(xs, ys_err, c=COLORS[0], s=5, label='y')\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cca4eb9-905e-475c-853f-b6f5ab870f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5), layout='constrained')\n",
    "plot_tree_perf(axs, xs, ys, ys_pred1, ys_err1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff54884-c2dc-4a2a-a32a-3c48f6635ad4",
   "metadata": {},
   "source": [
    "### Pair of Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da707ad1-fb1a-4a3b-a4b9-fdc660b5c2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg2 = DecisionTreeRegressor(max_depth=1, random_state=42)\n",
    "reg2.fit(xs[:, None], ys_err1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828bd55e-55f6-4ac7-b2e7-dba4c549e232",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_pred2 = reg2.predict(xs[:, None])\n",
    "ys_err2 = ys - (ys_pred1 + ys_pred2)\n",
    "score = mean_squared_error(err, ys_pred2)\n",
    "print(f'mse is {score:e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abad942-0904-4165-9d7f-0a9b77e2e646",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5), layout='constrained')\n",
    "plot_tree_perf(axs, xs, ys_err1, ys_pred2, ys_err2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1715b839-c404-47da-9cc1-49fa07083294",
   "metadata": {},
   "source": [
    "The first regression model predicts target variable $y$\n",
    "\n",
    "$$\n",
    "    \\hat{y}_1 \\sim y.\n",
    "$$\n",
    "\n",
    "But the second on predicts residuals\n",
    "\n",
    "$$\n",
    "    \\hat{y}_2 \\sim y - \\hat{y}_1.\n",
    "$$\n",
    "\n",
    "So the ultimate predictions is \n",
    "\n",
    "$$\n",
    "    y \\sim \\hat{y}_1 + \\hat{y}_2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9837bc-e104-4926-8298-109d7eabd76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_pred = ys_pred1 + ys_pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34f6671-cbc1-4612-a39c-0d5343508a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(xs, ys, c=COLORS[0], s=5, label='y')\n",
    "plt.plot(xs, ys_pred, c=COLORS[1], label=r'$y_{pred}$')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ada00bc-b850-475f-a5f8-d61ce9d1877a",
   "metadata": {},
   "source": [
    "### Multiple Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a32435-4d90-434d-901f-92648b18734a",
   "metadata": {},
   "source": [
    "We can repeat this procedure further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aabe734-22d2-4058-87f0-60a96b0a8e7b",
   "metadata": {},
   "source": [
    "## Random Forest vs Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b067f43-e561-44db-9b6b-9c2b17152ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ba0e28-6a2d-4919-b486-54450642b7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e742b7-fe87-4aa6-8c37-e314688c1dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators_list = [1, 2, 3, 5, 10, 100, 200]\n",
    "nrows = len(n_estimators_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c003abb7-591a-4e83-bbb1-e2d07906fa4c",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ffb581-c7d1-4519-bf7d-4418edb1d806",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=nrows, ncols=2, figsize=(15, 5 * nrows), layout='constrained')\n",
    "\n",
    "for i, n_estimators in enumerate(tqdm(n_estimators_list)):\n",
    "    rf = RandomForestRegressor(n_estimators=n_estimators, max_depth=1)\n",
    "    rf.fit(xs[:, None], ys)\n",
    "    \n",
    "    ys_pred = rf.predict(xs[:, None])\n",
    "    ys_err = ys - ys_pred\n",
    "    \n",
    "    plot_tree_perf(axs[i], xs, ys, ys_pred, ys_err)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3096d647-5fa8-49b5-943e-ce33a7f2c94b",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da86312-90e2-4b4e-9871-1ee5f4277e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=nrows, ncols=2, figsize=(15, 5 * nrows), layout='constrained')\n",
    "\n",
    "for i, n_estimators in enumerate(tqdm(n_estimators_list)):\n",
    "    gb = GradientBoostingRegressor(n_estimators=n_estimators, max_depth=1, learning_rate=1)\n",
    "    gb.fit(xs[:, None], ys)\n",
    "    \n",
    "    ys_pred = gb.predict(xs[:, None])\n",
    "    ys_err = ys - ys_pred\n",
    "    \n",
    "    plot_tree_perf(axs[i], xs, ys, ys_pred, ys_err)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a862276-8f1d-44cc-9d07-79aadadec865",
   "metadata": {},
   "source": [
    "#### Complexoty and Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f422c66-1a03-48ae-b237-b9bd75194431",
   "metadata": {},
   "source": [
    "Now, let's see how different ensembel methods behave as number of estimator is increased in ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f40c680-ba92-4808-b98a-5f401f4d7394",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f082e3a-ac26-4cc7-98d1-22a8af887663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_parametric(clf, xs, ys, n_estimators_list: list[int]) -> np.ndarray:\n",
    "    xs_train, xs_test, ys_train, ys_test = train_test_split(xs, ys, test_size=1/3, random_state=42)\n",
    "\n",
    "    scores = np.empty((len(n_estimators_list), 2))\n",
    "    for i, n_estimators in enumerate(tqdm(n_estimators_list)):\n",
    "        rf = clf.set_params(n_estimators=n_estimators)\n",
    "        rf.fit(xs_train[:, None], ys_train)\n",
    "        \n",
    "        ys_pred = rf.predict(xs_train[:, None])\n",
    "        scores[i, 0] = mean_squared_error(ys_train, ys_pred)\n",
    "\n",
    "        ys_pred = rf.predict(xs_test[:, None])\n",
    "        scores[i, 1] = mean_squared_error(ys_test, ys_pred)\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3f5af0-57fe-4204-8b25-630edaed20b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators_list = [*range(1, 201)]  # Redefine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca443e18-4861-4414-9eaa-dd08b0ff535d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = partial(fit_parametric, xs=xs, ys=ys, n_estimators_list=n_estimators_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3517a7-e65f-483b-92db-3c11f977cc0c",
   "metadata": {},
   "source": [
    "Fit and apply RF/GB with difference size of ensemble to the same data split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e014a251-1365-4f80-b14e-9fc4b39c1c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(max_depth=1, random_state=42)\n",
    "rf_scores = fit(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d2ea88-da1b-4a5c-8d93-6cf688cdd117",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingRegressor(max_depth=1, learning_rate=1, random_state=42)\n",
    "gb_scores = fit(gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89e0f51-5a3b-4bad-90e8-3b2aab7f2b13",
   "metadata": {},
   "source": [
    "Let's take a look at how size of ensemble influences the performance of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0409adac-980b-42da-8ced-7d1081de0335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_complexity(ax, n_estimators_list, scores, title=None):\n",
    "    for i, label in enumerate(['train', 'test']):\n",
    "        ax.plot(n_estimators_list, scores[:, i], label=label)\n",
    "    ax.set_xlabel('#estimators (ensemble size)')\n",
    "    ax.set_ylabel('MSE')\n",
    "    if title is not None:\n",
    "        ax.set_title('Random Forest')\n",
    "    ax.grid(True)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7179107-4c0b-4544-a5bc-abcb4ea505a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 5), layout='constrained', sharex=True, sharey=True)\n",
    "plot_complexity(axs[0], n_estimators_list, rf_scores, 'Random Forest')\n",
    "plot_complexity(axs[1], n_estimators_list, gb_scores, 'Gradient Boosting')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa37361f-485a-46fd-aac6-9740e323d04b",
   "metadata": {},
   "source": [
    "## Production-Level Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d105458a-1b91-4efe-81f8-c3f0a95a3cff",
   "metadata": {},
   "source": [
    "Gradient boosting is one of the most used machine learning algorithm in practice for more then two decades. There are plenty of known world-wide companies which are used gradient boosting in their products. They have a billions requests and users and terrabyte-scale datasets. They need efficient and scalable algorithms to train a model and to apply in inference time.\n",
    "\n",
    "There are three major libraries which suggests its own flavor of grading boosting over decision trees.\n",
    "\n",
    "- [XGBoost][1] (stands for eXtreme Gradient Boosting).\n",
    "- [CatBoost][2] (aka Categorical Boosting) by Yandex.\n",
    "- [LightGBM][3] (LGBM for brevity) by MicroSoft.\n",
    "\n",
    "[1]: https://github.com/dmlc/xgboost\n",
    "[2]: https://github.com/catboost/catboost\n",
    "[3]: https://github.com/Microsoft/LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abb4479-6b87-4e87-a85f-f8ea76efc2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install catboost lightgbm xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34273060-f9d8-4ada-916e-3a601d6a7239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ad0776-edfe-4e03-a818-a5538c271c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['data'], data['target'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ac05d5-b976-4384-bf7f-536ec04dd14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6aed60-3fd4-4a05-ad1e-01f6f025a52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bst = XGBClassifier(n_estimators=5, max_depth=2, learning_rate=1, objective='binary:logistic')\n",
    "bst.fit(X_train, y_train)\n",
    "y_pred = bst.predict(X_test)\n",
    "score = bst.score(X_test, y_test)\n",
    "print(f'xgboost accuracy is {score * 100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabb6e83-0fe6-449f-952d-99ec117b7111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5224116-8489-4271-b7ae-5696d542a357",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X_train, columns=data.feature_names)\n",
    "df['dummy'] = pd.Categorical(np.random.randint(0, 3, size=X_train.shape[0]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f30b33c-6438-4d26-95f3-eaf797ec64b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = Pool(data=df.to_dict('split')['data'], label=y_train, cat_features=[4])\n",
    "pool.get_cat_feature_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3097008c-a588-48c7-87df-0addaab21ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cb = CatBoostClassifier(iterations=5, depth=2, learning_rate=1, loss_function='MultiClass', verbose=True)\n",
    "cb.fit(X_train, y_train, plot=True)\n",
    "preds = cb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1a175a-257e-45fe-91b2-10a0c137fbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a830b172-090d-4f13-8411-de43935dd022",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset(data=df, label=y_train, categorical_feature=['dummy'])\n",
    "ds.categorical_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6ed9b0-2da9-4eb1-a2a8-804c2fa9d206",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lgb = LGBMClassifier(n_estimators=5, learning_rate=1, max_depth=2)\n",
    "lgb.fit(X_train, y_train)\n",
    "y_pred = lgb.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
